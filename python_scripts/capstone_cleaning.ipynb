{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92e8ffc1-43c7-42e7-acb9-bffef9339647",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8f236bb-5784-40f5-aa42-ca0681c16cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_folder = 'Project_Rawdata/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de72b19d-fffe-4938-892d-03bf99578260",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = [f for f in os.listdir(raw_data_folder) if f.endswith('.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfc6ec3d-ca01-4599-b071-eed3743aecff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 CSV files in 'Project_Rawdata/':\n",
      "- 202501-divvy-tripdata.csv\n",
      "- 202502-divvy-tripdata.csv\n",
      "- 202503-divvy-tripdata.csv\n",
      "- 202504-divvy-tripdata.csv\n",
      "- 202505-divvy-tripdata.csv\n",
      "- 202506-divvy-tripdata.csv\n"
     ]
    }
   ],
   "source": [
    "print(f\"Found {len(csv_files)} CSV files in '{raw_data_folder}':\")\n",
    "for f in csv_files:\n",
    "    print(f\"- {f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc8cf6fc-0f4c-4819-858a-80718849ffa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06253387-af81-4a9e-a44d-68b662b57e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Project_Rawdata/202501-divvy-tripdata.csv...\n",
      "Reading Project_Rawdata/202502-divvy-tripdata.csv...\n",
      "Reading Project_Rawdata/202503-divvy-tripdata.csv...\n",
      "Reading Project_Rawdata/202504-divvy-tripdata.csv...\n",
      "Reading Project_Rawdata/202505-divvy-tripdata.csv...\n",
      "Reading Project_Rawdata/202506-divvy-tripdata.csv...\n"
     ]
    }
   ],
   "source": [
    "for file_name in csv_files:\n",
    "    file_path = os.path.join(raw_data_folder, file_name)\n",
    "    print(f\"Reading {file_path}...\")\n",
    "    df = pd.read_csv(file_path, low_memory=False)\n",
    "    all_dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c23db5b2-a7ef-4dcb-848a-ad7e79a48695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating all DataFrames...\n"
     ]
    }
   ],
   "source": [
    "print(\"Concatenating all DataFrames...\")\n",
    "combined_df = pd.concat(all_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd52b487-b34b-4d58-be24-8de6c6a93163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined DataFrame shape: (2141425, 13)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Combined DataFrame shape: {combined_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b870813a-0bf4-4b33-b837-b93396b0da2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            ride_id  rideable_type               started_at  \\\n",
      "0  7569BC890583FCD7   classic_bike  2025-01-21 17:23:54.538   \n",
      "1  013609308856B7FC  electric_bike  2025-01-11 15:44:06.795   \n",
      "2  EACACD3CE0607C0D   classic_bike  2025-01-02 15:16:27.730   \n",
      "3  EAA2485BA64710D3   classic_bike  2025-01-23 08:49:05.814   \n",
      "4  7F8BE2471C7F746B  electric_bike  2025-01-16 08:38:32.338   \n",
      "\n",
      "                  ended_at            start_station_name start_station_id  \\\n",
      "0  2025-01-21 17:37:52.015     Wacker Dr & Washington St     KA1503000072   \n",
      "1  2025-01-11 15:49:11.139   Halsted St & Wrightwood Ave     TA1309000061   \n",
      "2  2025-01-02 15:28:03.230  Southport Ave & Waveland Ave            13235   \n",
      "3  2025-01-23 08:52:40.047  Southport Ave & Waveland Ave            13235   \n",
      "4  2025-01-16 08:41:06.767  Southport Ave & Waveland Ave            13235   \n",
      "\n",
      "            end_station_name end_station_id  start_lat  start_lng    end_lat  \\\n",
      "0       McClurg Ct & Ohio St   TA1306000029  41.883143 -87.637242  41.892592   \n",
      "1   Racine Ave & Belmont Ave   TA1308000019  41.929147 -87.649153  41.939743   \n",
      "2    Broadway & Cornelia Ave          13278  41.948226 -87.664071  41.945529   \n",
      "3  Southport Ave & Roscoe St          13071  41.948226 -87.664071  41.943739   \n",
      "4  Southport Ave & Roscoe St          13071  41.948226 -87.664071  41.943739   \n",
      "\n",
      "     end_lng member_casual  \n",
      "0 -87.617289        member  \n",
      "1 -87.658865        member  \n",
      "2 -87.646439        member  \n",
      "3 -87.664020        member  \n",
      "4 -87.664020        member  \n"
     ]
    }
   ],
   "source": [
    "print(combined_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0db4f1f-2d8c-4eae-bdda-41f4b5817c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2141425 entries, 0 to 2141424\n",
      "Data columns (total 13 columns):\n",
      " #   Column              Dtype  \n",
      "---  ------              -----  \n",
      " 0   ride_id             object \n",
      " 1   rideable_type       object \n",
      " 2   started_at          object \n",
      " 3   ended_at            object \n",
      " 4   start_station_name  object \n",
      " 5   start_station_id    object \n",
      " 6   end_station_name    object \n",
      " 7   end_station_id      object \n",
      " 8   start_lat           float64\n",
      " 9   start_lng           float64\n",
      " 10  end_lat             float64\n",
      " 11  end_lng             float64\n",
      " 12  member_casual       object \n",
      "dtypes: float64(4), object(9)\n",
      "memory usage: 212.4+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(combined_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48181ec0-08cb-4443-b674-3e2c9a687045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the combined DataFrame: (2141425, 13)\n",
      "\n",
      "Missing values (NaN/NaT) per column:\n",
      "ride_id                    0\n",
      "rideable_type              0\n",
      "started_at                 0\n",
      "ended_at                   0\n",
      "start_station_name    429577\n",
      "start_station_id      429577\n",
      "end_station_name      447149\n",
      "end_station_id        447149\n",
      "start_lat                  0\n",
      "start_lng                  0\n",
      "end_lat                 2187\n",
      "end_lng                 2187\n",
      "member_casual              0\n",
      "dtype: int64\n",
      "\n",
      "Percentage of missing values per column:\n",
      "ride_id                0.000000\n",
      "rideable_type          0.000000\n",
      "started_at             0.000000\n",
      "ended_at               0.000000\n",
      "start_station_name    20.060334\n",
      "start_station_id      20.060334\n",
      "end_station_name      20.880909\n",
      "end_station_id        20.880909\n",
      "start_lat              0.000000\n",
      "start_lng              0.000000\n",
      "end_lat                0.102128\n",
      "end_lng                0.102128\n",
      "member_casual          0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Null values will be handled in this part:\n",
    "print(\"Shape of the combined DataFrame:\", combined_df.shape)\n",
    "print(\"\\nMissing values (NaN/NaT) per column:\")\n",
    "print(combined_df.isnull().sum())\n",
    "\n",
    "print(\"\\nPercentage of missing values per column:\")\n",
    "print((combined_df.isnull().sum() / len(combined_df)) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab113f98-2c29-466c-a6b9-62c200a1a6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Critical columns: ride_id, rideable_type, started_at, ended_at and member_casual are very important in this analysis. Thankfully, none is null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ea07398-ba91-459f-b500-286a738426dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normally, we could fix values at column end_lat and end_lng to 0 but distance based analysis,\n",
    "# Could give us some hidden information about users preference on membership type. \n",
    "# Thus, minor amount of null values in those columns will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b5fce6b-c985-4d14-8936-fc439e293816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copy of dataframe to handle cleaning without any data loss\n",
    "df_cleaned = combined_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51c27875-cc15-455e-ab67-be9c29f07f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['end_lat', 'end_lng']\n",
    "df_cleaned.dropna(subset=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b581a6e-974c-416a-9f0c-7fbb44959375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of missing values per column:\n",
      "ride_id                0.000000\n",
      "rideable_type          0.000000\n",
      "started_at             0.000000\n",
      "ended_at               0.000000\n",
      "start_station_name    20.060334\n",
      "start_station_id      20.060334\n",
      "end_station_name      20.778780\n",
      "end_station_id        20.778780\n",
      "start_lat              0.000000\n",
      "start_lng              0.000000\n",
      "end_lat                0.000000\n",
      "end_lng                0.000000\n",
      "member_casual          0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Check if null values dropped:\n",
    "print(\"Percentage of missing values per column:\")\n",
    "print((df_cleaned.isnull().sum() / len(combined_df)) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb08b1cf-ac06-4e77-bcf6-2c18656b9b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle nulls in station information columns:\n",
    "# These often have a lot of nulls, dropping them is not practical.\n",
    "# We'll fill them with 'UNKNOWN' to keep the rows.\n",
    "station_columns = ['start_station_name', 'start_station_id', 'end_station_name', 'end_station_id']\n",
    "\n",
    "for col in ['start_station_name', 'start_station_id', 'end_station_name', 'end_station_id']:\n",
    "    if col in df_cleaned.columns:\n",
    "        df_cleaned[col] = df_cleaned[col].fillna('UNKNOWN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "abc094b5-6033-4f82-b01c-d35495f29be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values after initial null handling:\n",
      "ride_id               0\n",
      "rideable_type         0\n",
      "started_at            0\n",
      "ended_at              0\n",
      "start_station_name    0\n",
      "start_station_id      0\n",
      "end_station_name      0\n",
      "end_station_id        0\n",
      "start_lat             0\n",
      "start_lng             0\n",
      "end_lat               0\n",
      "end_lng               0\n",
      "member_casual         0\n",
      "dtype: int64\n",
      "\n",
      "Percentage of missing values after initial null handling:\n",
      "ride_id               0.0\n",
      "rideable_type         0.0\n",
      "started_at            0.0\n",
      "ended_at              0.0\n",
      "start_station_name    0.0\n",
      "start_station_id      0.0\n",
      "end_station_name      0.0\n",
      "end_station_id        0.0\n",
      "start_lat             0.0\n",
      "start_lng             0.0\n",
      "end_lat               0.0\n",
      "end_lng               0.0\n",
      "member_casual         0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Verify nulls after handling\n",
    "print(\"\\nMissing values after initial null handling:\")\n",
    "print(df_cleaned.isnull().sum())\n",
    "print(\"\\nPercentage of missing values after initial null handling:\")\n",
    "print((df_cleaned.isnull().sum() / len(df_cleaned)) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "05f62a0e-646a-4c74-a5a1-f9105d6c0838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dtypes before conversion:\n",
      "ride_id                object\n",
      "rideable_type          object\n",
      "started_at             object\n",
      "ended_at               object\n",
      "start_station_name     object\n",
      "start_station_id       object\n",
      "end_station_name       object\n",
      "end_station_id         object\n",
      "start_lat             float64\n",
      "start_lng             float64\n",
      "end_lat               float64\n",
      "end_lng               float64\n",
      "member_casual          object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Data types handling:\n",
    "print(\"Dtypes before conversion:\")\n",
    "print(df_cleaned.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5425a012-1fea-46bd-9ab9-d09f1cdd0ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# started_at and ended_at columns should be converted to datetime format:\n",
    "# Using errors='coerce' will turn any unparseable dates into NaT (Not a Time),\n",
    "# which is good for identifying and potentially handling malformed dates.\n",
    "df_cleaned['started_at'] = pd.to_datetime(df_cleaned['started_at'], errors='coerce')\n",
    "df_cleaned['ended_at'] = pd.to_datetime(df_cleaned['ended_at'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e88b138-140c-44dc-8ad3-68f5b04a5a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values after initial dtype handling:\n",
      "ride_id               0\n",
      "rideable_type         0\n",
      "started_at            0\n",
      "ended_at              0\n",
      "start_station_name    0\n",
      "start_station_id      0\n",
      "end_station_name      0\n",
      "end_station_id        0\n",
      "start_lat             0\n",
      "start_lng             0\n",
      "end_lat               0\n",
      "end_lng               0\n",
      "member_casual         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Lets check if converting is done without any errors. Problematic convertions should be appear as null values:\n",
    "print(\"\\nMissing values after initial dtype handling:\")\n",
    "print(df_cleaned.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "16166a3f-c1f9-4e35-bba7-8a00a85dc569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'rideable_type' and 'member_casual' to 'category'\n",
    "categorical_cols = ['rideable_type', 'member_casual']\n",
    "for col in categorical_cols:\n",
    "    if col in df_cleaned.columns:\n",
    "        df_cleaned[col] = df_cleaned[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d6f4d78d-38ea-442d-9e91-06c413ecceb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values after initial dtype handling:\n",
      "ride_id               0\n",
      "rideable_type         0\n",
      "started_at            0\n",
      "ended_at              0\n",
      "start_station_name    0\n",
      "start_station_id      0\n",
      "end_station_name      0\n",
      "end_station_id        0\n",
      "start_lat             0\n",
      "start_lng             0\n",
      "end_lat               0\n",
      "end_lng               0\n",
      "member_casual         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Lets check if converting is done without any errors. Problematic convertions should be appear as null values:\n",
    "print(\"\\nMissing values after initial dtype handling:\")\n",
    "print(df_cleaned.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6582a6e4-6227-4953-8d8c-b5474ca4593a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Dtypes after conversion:\n",
      "ride_id               string[python]\n",
      "rideable_type               category\n",
      "started_at            datetime64[ns]\n",
      "ended_at              datetime64[ns]\n",
      "start_station_name    string[python]\n",
      "start_station_id      string[python]\n",
      "end_station_name      string[python]\n",
      "end_station_id        string[python]\n",
      "start_lat                    float64\n",
      "start_lng                    float64\n",
      "end_lat                      float64\n",
      "end_lng                      float64\n",
      "member_casual               category\n",
      "dtype: object\n",
      "\n",
      "Missing values after type conversion (checking for NaT if coerce was used):\n",
      "ride_id               0\n",
      "rideable_type         0\n",
      "started_at            0\n",
      "ended_at              0\n",
      "start_station_name    0\n",
      "start_station_id      0\n",
      "end_station_name      0\n",
      "end_station_id        0\n",
      "start_lat             0\n",
      "start_lng             0\n",
      "end_lat               0\n",
      "end_lng               0\n",
      "member_casual         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Convert other 'object' columns to 'string' dtype\n",
    "string_cols = [\n",
    "    'ride_id',\n",
    "    'start_station_name',\n",
    "    'start_station_id',\n",
    "    'end_station_name',\n",
    "    'end_station_id'\n",
    "]\n",
    "for col in string_cols:\n",
    "    if col in df_cleaned.columns and df_cleaned[col].dtype == 'object':\n",
    "        df_cleaned[col] = df_cleaned[col].astype('string')\n",
    "\n",
    "print(\"\\n\\nDtypes after conversion:\")\n",
    "print(df_cleaned.dtypes)\n",
    "\n",
    "# Verify any NaT values\n",
    "print(\"\\nMissing values after type conversion (checking for NaT if coerce was used):\")\n",
    "print(df_cleaned.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5894f798-4438-4b4e-8ed1-d6caf6aaa11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame before checking for duplicates: (2139238, 13)\n",
      "\n",
      "No duplicate ride_ids found. Dataset is unique based on ride_id.\n"
     ]
    }
   ],
   "source": [
    "# Duplicate handling is crucial before importing our dataset to sql:\n",
    "\n",
    "print(\"Shape of DataFrame before checking for duplicates:\", df_cleaned.shape)\n",
    "\n",
    "# Check for duplicate ride_ids\n",
    "# Keep=False marks ALL occurrences of a duplicate as True\n",
    "duplicate_ride_ids = df_cleaned[df_cleaned.duplicated(subset=['ride_id'], keep=False)]\n",
    "\n",
    "if not duplicate_ride_ids.empty:\n",
    "    print(f\"\\n--- Found {len(duplicate_ride_ids)} rows with duplicate ride_ids. ---\")\n",
    "    print(\"\\nSample of duplicate ride_ids (showing all occurrences):\")\n",
    "    print(duplicate_ride_ids.sort_values('ride_id').head(10)) # Display first 10 duplicate rows\n",
    "else:\n",
    "    print(\"\\nNo duplicate ride_ids found. Dataset is unique based on ride_id.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "25205f02-0b52-4128-8223-b0f475be7e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are no duplicates in our primary key column. This is great. Now we can move on to data manipulation to handle outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ced92a6a-4cdf-465a-8ae6-6434d82d4078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame before handling ride duration outliers: (2139238, 13)\n",
      "\n",
      "Removed 52106 rides due to duration outliers.\n",
      "New DataFrame shape after duration filtering: (2087132, 14)\n",
      "\n",
      "Min ride duration after filtering: 1.00 minutes\n",
      "Max ride duration after filtering: 1435.21 minutes\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of DataFrame before handling ride duration outliers:\", df_cleaned.shape)\n",
    "\n",
    "# 1. Calculate ride_length in minutes\n",
    "df_cleaned['ride_length_minutes'] = (df_cleaned['ended_at'] - df_cleaned['started_at']).dt.total_seconds() / 60\n",
    "\n",
    "# Check for negative or zero ride lengths (should ideally be handled by >= 1 min filter, but good to inspect)\n",
    "negative_rides = df_cleaned[df_cleaned['ride_length_minutes'] <= 0]\n",
    "if not negative_rides.empty:\n",
    "    print(f\"\\nFound {len(negative_rides)} rides with non-positive duration. These will be removed.\")\n",
    "    \n",
    "# 2. Define outlier thresholds\n",
    "min_ride_minutes = 1  # 1 minute\n",
    "max_ride_minutes = 1440 # 24 hours\n",
    "\n",
    "# 3. Filter the DataFrame to keep only valid rides\n",
    "initial_rides = df_cleaned.shape[0]\n",
    "df_cleaned = df_cleaned[\n",
    "    (df_cleaned['ride_length_minutes'] >= min_ride_minutes) &\n",
    "    (df_cleaned['ride_length_minutes'] <= max_ride_minutes)\n",
    "]\n",
    "rides_after_duration_filter = df_cleaned.shape[0]\n",
    "\n",
    "print(f\"\\nRemoved {initial_rides - rides_after_duration_filter} rides due to duration outliers.\")\n",
    "print(f\"New DataFrame shape after duration filtering: {df_cleaned.shape}\")\n",
    "\n",
    "# Quick check on min/max duration\n",
    "print(f\"\\nMin ride duration after filtering: {df_cleaned['ride_length_minutes'].min():.2f} minutes\")\n",
    "print(f\"Max ride duration after filtering: {df_cleaned['ride_length_minutes'].max():.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "83408f2c-48c8-4966-900a-f240572093b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that outliers in ride lenght are handled, we can move on to handle very long, unrealistic rides to handle them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a5dfe700-1e49-4f25-ad68-8aaf515f7d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of DataFrame before handling geographical outliers: (2087132, 14)\n",
      "\n",
      "Removed 2 rides due to geographical outliers.\n",
      "New DataFrame shape after geographical filtering: (2087130, 14)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nShape of DataFrame before handling geographical outliers:\", df_cleaned.shape)\n",
    "\n",
    "# Approximate geographical bounds for Chicago area\n",
    "chicago_lat_min = 41.5\n",
    "chicago_lat_max = 42.5\n",
    "chicago_lng_min = -88.0\n",
    "chicago_lng_max = -87.0\n",
    "\n",
    "initial_rides = df_cleaned.shape[0]\n",
    "\n",
    "# Filter out rides where start or end coordinates are outside reasonable Chicago bounds\n",
    "df_cleaned = df_cleaned[\n",
    "    (df_cleaned['start_lat'] >= chicago_lat_min) & (df_cleaned['start_lat'] <= chicago_lat_max) &\n",
    "    (df_cleaned['start_lng'] >= chicago_lng_min) & (df_cleaned['start_lng'] <= chicago_lng_max) &\n",
    "    (df_cleaned['end_lat'] >= chicago_lat_min) & (df_cleaned['end_lat'] <= chicago_lat_max) &\n",
    "    (df_cleaned['end_lng'] >= chicago_lng_min) & (df_cleaned['end_lng'] <= chicago_lng_max)\n",
    "]\n",
    "\n",
    "rides_after_geo_filter = df_cleaned.shape[0]\n",
    "\n",
    "print(f\"\\nRemoved {initial_rides - rides_after_geo_filter} rides due to geographical outliers.\")\n",
    "print(f\"New DataFrame shape after geographical filtering: {df_cleaned.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "182c93a4-d76d-4f27-8bd9-f8fd8580db2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Category Dtype Entry Check ---\n",
      "\n",
      "Unique values and counts for 'rideable_type':\n",
      "rideable_type\n",
      "electric_bike    1297179\n",
      "classic_bike      789951\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Unique values and counts for 'member_casual':\n",
      "member_casual\n",
      "member    1394304\n",
      "casual     692826\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Lets handle outliers in bike types and membership types:\n",
    "# Ideally there should be two types of bikes and two types of memberships\n",
    "\n",
    "print(\"\\n--- Category Dtype Entry Check ---\")\n",
    "\n",
    "# Check 'rideable_type'\n",
    "print(\"\\nUnique values and counts for 'rideable_type':\")\n",
    "print(df_cleaned['rideable_type'].value_counts(dropna=False))\n",
    "\n",
    "# Check 'member_casual'\n",
    "print(\"\\nUnique values and counts for 'member_casual':\")\n",
    "print(df_cleaned['member_casual'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ffd6c54c-c299-403b-9663-19f84e46b1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            ride_id  rideable_type              started_at  \\\n",
      "0  7569BC890583FCD7   classic_bike 2025-01-21 17:23:54.538   \n",
      "1  013609308856B7FC  electric_bike 2025-01-11 15:44:06.795   \n",
      "2  EACACD3CE0607C0D   classic_bike 2025-01-02 15:16:27.730   \n",
      "3  EAA2485BA64710D3   classic_bike 2025-01-23 08:49:05.814   \n",
      "4  7F8BE2471C7F746B  electric_bike 2025-01-16 08:38:32.338   \n",
      "\n",
      "                 ended_at            start_station_name start_station_id  \\\n",
      "0 2025-01-21 17:37:52.015     Wacker Dr & Washington St     KA1503000072   \n",
      "1 2025-01-11 15:49:11.139   Halsted St & Wrightwood Ave     TA1309000061   \n",
      "2 2025-01-02 15:28:03.230  Southport Ave & Waveland Ave            13235   \n",
      "3 2025-01-23 08:52:40.047  Southport Ave & Waveland Ave            13235   \n",
      "4 2025-01-16 08:41:06.767  Southport Ave & Waveland Ave            13235   \n",
      "\n",
      "            end_station_name end_station_id  start_lat  start_lng    end_lat  \\\n",
      "0       McClurg Ct & Ohio St   TA1306000029  41.883143 -87.637242  41.892592   \n",
      "1   Racine Ave & Belmont Ave   TA1308000019  41.929147 -87.649153  41.939743   \n",
      "2    Broadway & Cornelia Ave          13278  41.948226 -87.664071  41.945529   \n",
      "3  Southport Ave & Roscoe St          13071  41.948226 -87.664071  41.943739   \n",
      "4  Southport Ave & Roscoe St          13071  41.948226 -87.664071  41.943739   \n",
      "\n",
      "     end_lng member_casual  ride_length_minutes  \n",
      "0 -87.617289        member            13.957950  \n",
      "1 -87.658865        member             5.072400  \n",
      "2 -87.646439        member            11.591667  \n",
      "3 -87.664020        member             3.570550  \n",
      "4 -87.664020        member             2.573817  \n"
     ]
    }
   ],
   "source": [
    "# Lets check the dataset\n",
    "print(df_cleaned.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "318dea8a-7f7f-43eb-bde0-d63e4fce32e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we can see, ride_lenght_minutes part is not so easy to read. We can round the values to only have 1 numerical after the delimiter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dc496a27-0e98-4186-beb4-70741a94dbf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renamed 'ride_length_minutes' to 'trip_duration_mins'.\n",
      "\n",
      "First 5 rows with 'trip_duration_mins':\n",
      "               started_at                ended_at  trip_duration_mins\n",
      "0 2025-01-21 17:23:54.538 2025-01-21 17:37:52.015           13.957950\n",
      "1 2025-01-11 15:44:06.795 2025-01-11 15:49:11.139            5.072400\n",
      "2 2025-01-02 15:16:27.730 2025-01-02 15:28:03.230           11.591667\n",
      "3 2025-01-23 08:49:05.814 2025-01-23 08:52:40.047            3.570550\n",
      "4 2025-01-16 08:38:32.338 2025-01-16 08:41:06.767            2.573817\n"
     ]
    }
   ],
   "source": [
    "# First, lets change the column name:\n",
    "if 'ride_length_minutes' in df_cleaned.columns and 'trip_duration_mins' not in df_cleaned.columns:\n",
    "    df_cleaned.rename(columns={'ride_length_minutes': 'trip_duration_mins'}, inplace=True)\n",
    "    print(\"Renamed 'ride_length_minutes' to 'trip_duration_mins'.\")\n",
    "else:\n",
    "    print(\"'trip_duration_mins' column already exists.\")\n",
    "\n",
    "print(\"\\nFirst 5 rows with 'trip_duration_mins':\")\n",
    "print(df_cleaned[['started_at', 'ended_at', 'trip_duration_mins']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7c4b44d8-6ac4-49ae-84f7-68a35f28b186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated 'trip_duration_mins' column (first 5 rows):\n",
      "               started_at                ended_at  trip_duration_mins\n",
      "0 2025-01-21 17:23:54.538 2025-01-21 17:37:52.015                14.0\n",
      "1 2025-01-11 15:44:06.795 2025-01-11 15:49:11.139                 5.1\n",
      "2 2025-01-02 15:16:27.730 2025-01-02 15:28:03.230                11.6\n",
      "3 2025-01-23 08:49:05.814 2025-01-23 08:52:40.047                 3.6\n",
      "4 2025-01-16 08:38:32.338 2025-01-16 08:41:06.767                 2.6\n",
      "\n",
      "Min duration (rounded): 1.0 minutes\n",
      "Max duration (rounded): 1435.2 minutes\n"
     ]
    }
   ],
   "source": [
    "# Round the values in the 'trip_duration_mins' column to 1 decimal place\n",
    "df_cleaned['trip_duration_mins'] = df_cleaned['trip_duration_mins'].round(1)\n",
    "\n",
    "print(\"Updated 'trip_duration_mins' column (first 5 rows):\")\n",
    "print(df_cleaned[['started_at', 'ended_at', 'trip_duration_mins']].head())\n",
    "\n",
    "print(f\"\\nMin duration (rounded): {df_cleaned['trip_duration_mins'].min()} minutes\")\n",
    "print(f\"Max duration (rounded): {df_cleaned['trip_duration_mins'].max()} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1831dbf6-1dfb-41e7-8131-bdf4812307e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving df_cleaned to cleaned_data/cyclistic_trips_cleaned_temp.csv before kernel restart...\n",
      "df_cleaned saved to CSV successfully.\n"
     ]
    }
   ],
   "source": [
    "output_folder = 'cleaned_data/'\n",
    "output_file_csv = os.path.join(output_folder, 'cyclistic_trips_cleaned_temp.csv') # Use a temp name\n",
    "\n",
    "print(f\"Saving df_cleaned to {output_file_csv} before kernel restart...\")\n",
    "df_cleaned.to_csv(output_file_csv, index=False) # index=False is important\n",
    "print(\"df_cleaned saved to CSV successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c46ebf1d-91f7-471c-8e20-cdb37ca2e2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing pyarrow...\n",
      "Collecting pyarrow\n",
      "  Downloading pyarrow-21.0.0-cp313-cp313-win_amd64.whl.metadata (3.4 kB)\n",
      "Downloading pyarrow-21.0.0-cp313-cp313-win_amd64.whl (26.1 MB)\n",
      "   ---------------------------------------- 0.0/26.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/26.1 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 1.6/26.1 MB 7.0 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 1.8/26.1 MB 7.3 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 4.7/26.1 MB 7.6 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 6.6/26.1 MB 7.9 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 8.4/26.1 MB 8.1 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 9.7/26.1 MB 8.2 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 12.1/26.1 MB 8.4 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 13.9/26.1 MB 8.4 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 15.7/26.1 MB 8.5 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 17.6/26.1 MB 8.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 19.4/26.1 MB 8.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 21.2/26.1 MB 8.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 23.1/26.1 MB 8.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.9/26.1 MB 8.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 26.1/26.1 MB 8.5 MB/s eta 0:00:00\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-21.0.0\n",
      "Installing geopy...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting geopy\n",
      "  Downloading geopy-2.4.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting geographiclib<3,>=1.52 (from geopy)\n",
      "  Downloading geographiclib-2.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Downloading geopy-2.4.1-py3-none-any.whl (125 kB)\n",
      "Downloading geographiclib-2.0-py3-none-any.whl (40 kB)\n",
      "Installing collected packages: geographiclib, geopy\n",
      "Successfully installed geographiclib-2.0 geopy-2.4.1\n",
      "Installation commands sent. You may need to restart the kernel now.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# Install pyarrow\n",
    "print(\"Installing pyarrow...\")\n",
    "!{sys.executable} -m pip install pyarrow\n",
    "\n",
    "# Install geopy\n",
    "print(\"Installing geopy...\")\n",
    "!{sys.executable} -m pip install geopy\n",
    "\n",
    "print(\"Installation commands sent. You may need to restart the kernel now.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfc73a77-4497-4970-85eb-3cdb4fc7b93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading df_cleaned from cleaned_data/cyclistic_trips_cleaned_temp.csv after kernel restart...\n",
      "df_cleaned loaded successfully from CSV.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2087130 entries, 0 to 2087129\n",
      "Data columns (total 14 columns):\n",
      " #   Column              Dtype         \n",
      "---  ------              -----         \n",
      " 0   ride_id             string        \n",
      " 1   rideable_type       category      \n",
      " 2   started_at          datetime64[ns]\n",
      " 3   ended_at            datetime64[ns]\n",
      " 4   start_station_name  string        \n",
      " 5   start_station_id    string        \n",
      " 6   end_station_name    string        \n",
      " 7   end_station_id      string        \n",
      " 8   start_lat           float64       \n",
      " 9   start_lng           float64       \n",
      " 10  end_lat             float64       \n",
      " 11  end_lng             float64       \n",
      " 12  member_casual       category      \n",
      " 13  trip_duration_mins  float64       \n",
      "dtypes: category(2), datetime64[ns](2), float64(5), string(5)\n",
      "memory usage: 195.1 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "output_folder = 'cleaned_data/'\n",
    "output_file_csv = os.path.join(output_folder, 'cyclistic_trips_cleaned_temp.csv')\n",
    "\n",
    "print(f\"Loading df_cleaned from {output_file_csv} after kernel restart...\")\n",
    "df_cleaned = pd.read_csv(output_file_csv, low_memory=False) # low_memory=False is good practice\n",
    "\n",
    "# Re-apply critical type conversions if they were not saved correctly by to_csv\n",
    "# (datetime columns are often read back as strings from CSV)\n",
    "df_cleaned['started_at'] = pd.to_datetime(df_cleaned['started_at'], errors='coerce')\n",
    "df_cleaned['ended_at'] = pd.to_datetime(df_cleaned['ended_at'], errors='coerce')\n",
    "\n",
    "# Re-apply category conversions if they were not saved correctly by to_csv\n",
    "categorical_cols = ['rideable_type', 'member_casual']\n",
    "for col in categorical_cols:\n",
    "    if col in df_cleaned.columns:\n",
    "        df_cleaned[col] = df_cleaned[col].astype('category')\n",
    "\n",
    "# Re-apply string conversions if they were not saved correctly by to_csv\n",
    "string_cols = [\n",
    "    'ride_id',\n",
    "    'start_station_name',\n",
    "    'start_station_id',\n",
    "    'end_station_name',\n",
    "    'end_station_id'\n",
    "]\n",
    "for col in string_cols:\n",
    "    if col in df_cleaned.columns and df_cleaned[col].dtype == 'object':\n",
    "        df_cleaned[col] = df_cleaned[col].astype('string')\n",
    "\n",
    "print(\"df_cleaned loaded successfully from CSV.\")\n",
    "print(df_cleaned.info()) # Verify types and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "995e7902-5144-47ef-9b89-ad6c9a40f446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting distance calculation for 'distance_traveled_km' ---\n",
      "Shape of DataFrame before distance calculation: (2087130, 14)\n",
      "Applying distance calculation (this might take a few minutes for a large dataset)...\n",
      "\n",
      "Successfully calculated distance for 2087130 rides.\n",
      "Number of rides where distance could not be calculated (marked as NA): 0\n",
      "\n",
      "First 10 rows with new 'distance_traveled_km' column:\n",
      "   start_lat  start_lng    end_lat    end_lng  distance_traveled_km\n",
      "0  41.883143 -87.637242  41.892592 -87.617289                  1.96\n",
      "1  41.929147 -87.649153  41.939743 -87.658865                  1.43\n",
      "2  41.948226 -87.664071  41.945529 -87.646439                  1.49\n",
      "3  41.948226 -87.664071  41.943739 -87.664020                  0.50\n",
      "4  41.948226 -87.664071  41.943739 -87.664020                  0.50\n",
      "5  41.853780 -87.646603  41.857506 -87.645991                  0.42\n",
      "6  41.929143 -87.649077  41.928773 -87.663913                  1.23\n",
      "7  41.917741 -87.691392  41.882409 -87.639767                  5.81\n",
      "8  41.897764 -87.642884  41.891795 -87.658751                  1.47\n",
      "9  41.914027 -87.705126  41.924632 -87.689307                  1.76\n",
      "\n",
      "Summary of 'distance_traveled_km':\n",
      "count    2.087130e+06\n",
      "mean     2.205440e+00\n",
      "std      1.950165e+00\n",
      "min      0.000000e+00\n",
      "25%      9.400000e-01\n",
      "50%      1.610000e+00\n",
      "75%      2.850000e+00\n",
      "max      3.279000e+01\n",
      "Name: distance_traveled_km, dtype: float64\n",
      "\n",
      "Note: Some rides have a 0.0 km distance, indicating start and end points are the same.\n"
     ]
    }
   ],
   "source": [
    "from geopy.distance import geodesic\n",
    "import pandas as pd\n",
    "\n",
    "print(\"\\n--- Starting distance calculation for 'distance_traveled_km' ---\")\n",
    "print(\"Shape of DataFrame before distance calculation:\", df_cleaned.shape)\n",
    "\n",
    "# Create an empty column for distance, initialized with Pandas' NA (nullable type)\n",
    "# This allows us to explicitly mark rows where distance couldn't be calculated.\n",
    "df_cleaned['distance_traveled_km'] = pd.NA\n",
    "\n",
    "# It's good practice to ensure lat/lng are numeric (float) before calculations\n",
    "\n",
    "df_cleaned['start_lat'] = pd.to_numeric(df_cleaned['start_lat'], errors='coerce')\n",
    "df_cleaned['start_lng'] = pd.to_numeric(df_cleaned['start_lng'], errors='coerce')\n",
    "df_cleaned['end_lat'] = pd.to_numeric(df_cleaned['end_lat'], errors='coerce')\n",
    "df_cleaned['end_lng'] = pd.to_numeric(df_cleaned['end_lng'], errors='coerce')\n",
    "\n",
    "# Count rows with potentially invalid (0 or NaN) coordinates before calculation\n",
    "# These rows won't yield a valid distance.\n",
    "invalid_coords_mask = (\n",
    "    (df_cleaned['start_lat'].isna()) | (df_cleaned['start_lng'].isna()) |\n",
    "    (df_cleaned['end_lat'].isna()) | (df_cleaned['end_lng'].isna()) |\n",
    "    (df_cleaned['start_lat'] == 0.0) | (df_cleaned['start_lng'] == 0.0) |\n",
    "    (df_cleaned['end_lat'] == 0.0) | (df_cleaned['end_lng'] == 0.0)\n",
    ")\n",
    "num_rows_with_invalid_coords = invalid_coords_mask.sum()\n",
    "\n",
    "if num_rows_with_invalid_coords > 0:\n",
    "    print(f\"Warning: {num_rows_with_invalid_coords} rows have invalid (NaN or 0) coordinates. Distance will be marked as NA for these.\")\n",
    "\n",
    "# Define a function to calculate distance safely\n",
    "def calculate_distance(row):\n",
    "    # Check for valid coordinates (not NaN and not 0.0)\n",
    "    if (pd.notna(row['start_lat']) and row['start_lat'] != 0.0 and\n",
    "        pd.notna(row['start_lng']) and row['start_lng'] != 0.0 and\n",
    "        pd.notna(row['end_lat']) and row['end_lat'] != 0.0 and\n",
    "        pd.notna(row['end_lng']) and row['end_lng'] != 0.0):\n",
    "        \n",
    "        start_coords = (row['start_lat'], row['start_lng'])\n",
    "        end_coords = (row['end_lat'], row['end_lng'])\n",
    "        try:\n",
    "            # Calculate distance in kilometers and round to 2 decimal places for readability\n",
    "            distance_km = geodesic(start_coords, end_coords).km\n",
    "            return round(distance_km, 2)\n",
    "        except ValueError: # Catch potential errors from geodesic for very strange coords\n",
    "            return pd.NA\n",
    "    else:\n",
    "        return pd.NA # Return NA if coordinates are invalid\n",
    "\n",
    "# Apply the function to each row\n",
    "# This operation can take some time for millions of rows.\n",
    "print(\"Applying distance calculation (this might take a few minutes for a large dataset)...\")\n",
    "df_cleaned['distance_traveled_km'] = df_cleaned.apply(calculate_distance, axis=1)\n",
    "\n",
    "# Report on the results\n",
    "num_calculated_distances = df_cleaned['distance_traveled_km'].count() # count non-NA values\n",
    "print(f\"\\nSuccessfully calculated distance for {num_calculated_distances} rides.\")\n",
    "print(f\"Number of rides where distance could not be calculated (marked as NA): {df_cleaned['distance_traveled_km'].isna().sum()}\")\n",
    "\n",
    "print(\"\\nFirst 10 rows with new 'distance_traveled_km' column:\")\n",
    "print(df_cleaned[['start_lat', 'start_lng', 'end_lat', 'end_lng', 'distance_traveled_km']].head(10))\n",
    "\n",
    "print(f\"\\nSummary of 'distance_traveled_km':\")\n",
    "print(df_cleaned['distance_traveled_km'].describe())\n",
    "\n",
    "# A quick check on minimum distance. 0km is possible if start and end coords are identical\n",
    "if df_cleaned['distance_traveled_km'].min() == 0.0:\n",
    "    print(\"\\nNote: Some rides have a 0.0 km distance, indicating start and end points are the same.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a782fe53-21ca-4ef2-a9c6-b4ca5a80f7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            ride_id  rideable_type              started_at  \\\n",
      "0  7569BC890583FCD7   classic_bike 2025-01-21 17:23:54.538   \n",
      "1  013609308856B7FC  electric_bike 2025-01-11 15:44:06.795   \n",
      "2  EACACD3CE0607C0D   classic_bike 2025-01-02 15:16:27.730   \n",
      "3  EAA2485BA64710D3   classic_bike 2025-01-23 08:49:05.814   \n",
      "4  7F8BE2471C7F746B  electric_bike 2025-01-16 08:38:32.338   \n",
      "\n",
      "                 ended_at            start_station_name start_station_id  \\\n",
      "0 2025-01-21 17:37:52.015     Wacker Dr & Washington St     KA1503000072   \n",
      "1 2025-01-11 15:49:11.139   Halsted St & Wrightwood Ave     TA1309000061   \n",
      "2 2025-01-02 15:28:03.230  Southport Ave & Waveland Ave            13235   \n",
      "3 2025-01-23 08:52:40.047  Southport Ave & Waveland Ave            13235   \n",
      "4 2025-01-16 08:41:06.767  Southport Ave & Waveland Ave            13235   \n",
      "\n",
      "            end_station_name end_station_id  start_lat  start_lng    end_lat  \\\n",
      "0       McClurg Ct & Ohio St   TA1306000029  41.883143 -87.637242  41.892592   \n",
      "1   Racine Ave & Belmont Ave   TA1308000019  41.929147 -87.649153  41.939743   \n",
      "2    Broadway & Cornelia Ave          13278  41.948226 -87.664071  41.945529   \n",
      "3  Southport Ave & Roscoe St          13071  41.948226 -87.664071  41.943739   \n",
      "4  Southport Ave & Roscoe St          13071  41.948226 -87.664071  41.943739   \n",
      "\n",
      "     end_lng member_casual  trip_duration_mins  distance_traveled_km  \n",
      "0 -87.617289        member                14.0                  1.96  \n",
      "1 -87.658865        member                 5.1                  1.43  \n",
      "2 -87.646439        member                11.6                  1.49  \n",
      "3 -87.664020        member                 3.6                  0.50  \n",
      "4 -87.664020        member                 2.6                  0.50  \n"
     ]
    }
   ],
   "source": [
    "print(df_cleaned.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "654d47d5-bf8d-496a-810e-88e95c7081d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving df_cleaned to Parquet file: cleaned_data/cyclistic_trips_cleaned.parquet...\n",
      "df_cleaned successfully saved to Parquet.\n",
      "You can find the file at: C:\\Cyclistic_Capstone\\cleaned_data\\cyclistic_trips_cleaned.parquet\n"
     ]
    }
   ],
   "source": [
    "output_folder = 'cleaned_data/'\n",
    "final_parquet_path = os.path.join(output_folder, 'cyclistic_trips_cleaned.parquet')\n",
    "\n",
    "print(f\"Saving df_cleaned to Parquet file: {final_parquet_path}...\")\n",
    "\n",
    "# Save the DataFrame to Parquet\n",
    "# index=False is important to prevent Pandas from writing its DataFrame index as a column in the Parquet file\n",
    "df_cleaned.to_parquet(final_parquet_path, index=False)\n",
    "\n",
    "print(\"df_cleaned successfully saved to Parquet.\")\n",
    "print(f\"You can find the file at: {os.path.abspath(final_parquet_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a191f5e-cc2a-4fed-8c22-e4985fb1017e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing pyodbc...\n",
      "Requirement already satisfied: pyodbc in c:\\cyclistic_capstone\\venv\\lib\\site-packages (5.2.0)\n",
      "Installing sqlalchemy...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sqlalchemy\n",
      "  Downloading sqlalchemy-2.0.41-cp313-cp313-win_amd64.whl.metadata (9.8 kB)\n",
      "Collecting greenlet>=1 (from sqlalchemy)\n",
      "  Downloading greenlet-3.2.3-cp313-cp313-win_amd64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\cyclistic_capstone\\venv\\lib\\site-packages (from sqlalchemy) (4.14.1)\n",
      "Downloading sqlalchemy-2.0.41-cp313-cp313-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.1 MB ? eta -:--:--\n",
      "   ----------------------------- ---------- 1.6/2.1 MB 7.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 5.0 MB/s eta 0:00:00\n",
      "Downloading greenlet-3.2.3-cp313-cp313-win_amd64.whl (297 kB)\n",
      "Installing collected packages: greenlet, sqlalchemy\n",
      "Successfully installed greenlet-3.2.3 sqlalchemy-2.0.41\n",
      "Installation commands sent. You may need to restart the kernel now.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"Installing pyodbc...\")\n",
    "!{sys.executable} -m pip install pyodbc\n",
    "\n",
    "print(\"Installing sqlalchemy...\")\n",
    "!{sys.executable} -m pip install sqlalchemy\n",
    "\n",
    "print(\"Installation commands sent. You may need to restart the kernel now.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23e9b0f6-2069-4cfc-96fd-372f78b7ec41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading df_cleaned from Parquet file: cleaned_data/cyclistic_trips_cleaned.parquet...\n",
      "df_cleaned successfully loaded from Parquet.\n",
      "\n",
      "DataFrame Info after loading from Parquet:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2087130 entries, 0 to 2087129\n",
      "Data columns (total 15 columns):\n",
      " #   Column                Dtype         \n",
      "---  ------                -----         \n",
      " 0   ride_id               string        \n",
      " 1   rideable_type         category      \n",
      " 2   started_at            datetime64[ns]\n",
      " 3   ended_at              datetime64[ns]\n",
      " 4   start_station_name    string        \n",
      " 5   start_station_id      string        \n",
      " 6   end_station_name      string        \n",
      " 7   end_station_id        string        \n",
      " 8   start_lat             float64       \n",
      " 9   start_lng             float64       \n",
      " 10  end_lat               float64       \n",
      " 11  end_lng               float64       \n",
      " 12  member_casual         category      \n",
      " 13  trip_duration_mins    float64       \n",
      " 14  distance_traveled_km  float64       \n",
      "dtypes: category(2), datetime64[ns](2), float64(6), string(5)\n",
      "memory usage: 211.0 MB\n",
      "None\n",
      "\n",
      "First 5 rows of loaded DataFrame:\n",
      "            ride_id  rideable_type              started_at  \\\n",
      "0  7569BC890583FCD7   classic_bike 2025-01-21 17:23:54.538   \n",
      "1  013609308856B7FC  electric_bike 2025-01-11 15:44:06.795   \n",
      "2  EACACD3CE0607C0D   classic_bike 2025-01-02 15:16:27.730   \n",
      "3  EAA2485BA64710D3   classic_bike 2025-01-23 08:49:05.814   \n",
      "4  7F8BE2471C7F746B  electric_bike 2025-01-16 08:38:32.338   \n",
      "\n",
      "                 ended_at            start_station_name start_station_id  \\\n",
      "0 2025-01-21 17:37:52.015     Wacker Dr & Washington St     KA1503000072   \n",
      "1 2025-01-11 15:49:11.139   Halsted St & Wrightwood Ave     TA1309000061   \n",
      "2 2025-01-02 15:28:03.230  Southport Ave & Waveland Ave            13235   \n",
      "3 2025-01-23 08:52:40.047  Southport Ave & Waveland Ave            13235   \n",
      "4 2025-01-16 08:41:06.767  Southport Ave & Waveland Ave            13235   \n",
      "\n",
      "            end_station_name end_station_id  start_lat  start_lng    end_lat  \\\n",
      "0       McClurg Ct & Ohio St   TA1306000029  41.883143 -87.637242  41.892592   \n",
      "1   Racine Ave & Belmont Ave   TA1308000019  41.929147 -87.649153  41.939743   \n",
      "2    Broadway & Cornelia Ave          13278  41.948226 -87.664071  41.945529   \n",
      "3  Southport Ave & Roscoe St          13071  41.948226 -87.664071  41.943739   \n",
      "4  Southport Ave & Roscoe St          13071  41.948226 -87.664071  41.943739   \n",
      "\n",
      "     end_lng member_casual  trip_duration_mins  distance_traveled_km  \n",
      "0 -87.617289        member                14.0                  1.96  \n",
      "1 -87.658865        member                 5.1                  1.43  \n",
      "2 -87.646439        member                11.6                  1.49  \n",
      "3 -87.664020        member                 3.6                  0.50  \n",
      "4 -87.664020        member                 2.6                  0.50  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the output folder and file path where Parquet file is located\n",
    "output_folder = 'cleaned_data/'\n",
    "final_parquet_path = os.path.join(output_folder, 'cyclistic_trips_cleaned.parquet')\n",
    "\n",
    "# Check if the file exists before trying to load it\n",
    "if not os.path.exists(final_parquet_path):\n",
    "    print(f\"Error: Parquet file not found at {final_parquet_path}\")\n",
    "    print(\"Please ensure the file was saved correctly and the path is accurate.\")\n",
    "else:\n",
    "    print(f\"Loading df_cleaned from Parquet file: {final_parquet_path}...\")\n",
    "\n",
    "    # Read the Parquet file into a DataFrame\n",
    "    df_cleaned = pd.read_parquet(final_parquet_path)\n",
    "\n",
    "    print(\"df_cleaned successfully loaded from Parquet.\")\n",
    "\n",
    "    # Display basic info to verify data types and integrity\n",
    "    print(\"\\nDataFrame Info after loading from Parquet:\")\n",
    "    print(df_cleaned.info())\n",
    "\n",
    "    print(\"\\nFirst 5 rows of loaded DataFrame:\")\n",
    "    print(df_cleaned.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e0e597b-21d8-4090-9899-b01e49fc2cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can move on to importing our dataset to Microsoft SQL Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "520d0156-fc19-4892-895f-293929bb7515",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "from sqlalchemy import create_engine\n",
    "import urllib\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4187a1f6-6869-4a6e-9f41-29701b38af6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:29: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:29: SyntaxWarning: invalid escape sequence '\\S'\n",
      "C:\\Users\\eren_\\AppData\\Local\\Temp\\ipykernel_18980\\4108179022.py:29: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  print(\"2. Verify your 'server' name is correct (e.g., 'localhost', '.\\SQLEXPRESS').\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load 2087130 rows into SQL Server table 'BikeTrips' in database 'CYCLISTICBIKESHARE' using Windows Authentication...\n",
      "Successfully loaded 2087130 rows into SQL Server.\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration for SQL Server Connection (Windows Authentication) ---\n",
    "server = r'EREN_MONSTER\\EREN' # Fixed SyntaxWarning here\n",
    "database = 'CYCLISTICBIKESHARE'\n",
    "table_name = 'BikeTrips'\n",
    "\n",
    "# --- Create SQL Alchemy Engine ---\n",
    "conn_str = (\n",
    "    f'DRIVER={{ODBC Driver 18 for SQL Server}};'\n",
    "    f'SERVER={server};'\n",
    "    f'DATABASE={database};'\n",
    "    f'Trusted_Connection=yes;'\n",
    "    f'TrustServerCertificate=yes;'\n",
    ")\n",
    "\n",
    "engine = create_engine(f\"mssql+pyodbc:///?odbc_connect={conn_str}\")\n",
    "\n",
    "# --- Data Import ---\n",
    "print(f\"Attempting to load {df_cleaned.shape[0]} rows into SQL Server table '{table_name}' in database '{database}' using Windows Authentication...\")\n",
    "\n",
    "try:\n",
    "    df_cleaned.to_sql(table_name, con=engine, if_exists='append', index=False, chunksize=1000)\n",
    "\n",
    "    print(f\"Successfully loaded {df_cleaned.shape[0]} rows into SQL Server.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data to SQL Server: {e}\")\n",
    "    print(\"\\nTroubleshooting tips for Windows Authentication:\")\n",
    "    print(\"1. Ensure SQL Server is running and accessible.\")\n",
    "    print(\"2. Verify your 'server' name is correct (e.g., 'localhost', '.\\SQLEXPRESS').\")\n",
    "    print(\"3. Check that 'ODBC Driver 17 for SQL Server' is installed on your system.\")\n",
    "    print(\"4. Ensure the table schema in SQL Server exactly matches your DataFrame columns and types.\")\n",
    "    print(\"5. Verify the Windows user account running this Jupyter Notebook has permissions to connect to SQL Server and write to the database/table.\")\n",
    "    print(\"6. Check firewall settings if SQL Server is on a different machine.\")\n",
    "finally:\n",
    "    engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f737c1-f8df-4ad4-aa97-5a3d666d9411",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
